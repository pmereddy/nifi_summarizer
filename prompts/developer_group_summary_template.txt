Goal: Provide a detailed execution summary of the NiFi process group for a developer or NiFi engineer. This variant should equip a developer to recreate or maintain the flow with full knowledge of each step. The language can be technical since the reader is familiar with NiFi and data integration concepts. The summary must not gloss over any details of how the flow works. 

nodes:
{nodes_yaml}

edges:
{edges_yaml}


Instructions/Features to include:
Exact Sequence of Operations: Walk through the flow step-by-step in the order data moves. Start with each source processor, describing what data it pulls in and from where. Proceed in order to the next connected component as per the from_id -> to_id mapping, and continue until final outputs. This chronological ordering should reflect the actual NiFi data path exactly. Avoid combining or skipping steps; each processor and connection gets attention so the developer can trace the flow end-to-end.
Component Names and Functions: Identify each processor by its NiFi component name and a brief explanation of its function. For example: “GenerateTableFetch – performs an incremental SQL query to retrieve new customer records from the CRM database." Include relevant NiFi processor type names (like UpdateAttribute, RouteOnAttribute, etc.) since a developer will recognize these and it helps them map the summary to NiFi components. Make sure to mention processor groupings or labels if they indicate sub-sections of the flow (e.g., if the YAML defines sub-process groups or labels, use them to orient the reader).
Key Properties and Configurations: For each processor, list crucial properties or settings that affect logic. Include things like: the scheduling strategy (e.g., “runs every 5 min" or “triggered by incoming FlowFiles"), any filter or query being used (e.g., the content of a RouteOnAttribute rule or an SQL WHERE clause), and any special configuration. Even if a property is using a default value, mention it here for completeness – the idea is to document every operational detail. For example: “This processor is scheduled to run on the Primary Node only (to avoid duplicate processing in a cluster) and checks for new files every 60 seconds (default scheduling)." By including defaults (like default 1 GB or 10000 object back-pressure, default “do not load balance"), you reassure the developer that those settings were not accidentally overlooked.
Attribute Transformations and Evaluations: Explain how each step transforms the data or its attributes. If a processor adds or updates FlowFile attributes, state what it’s doing (e.g., “UpdateAttribute sets record.count to the number of records in the batch for logging"). If there is any conditional logic, detail how it’s evaluated: for instance, “RouteOnAttribute evaluates the customerType attribute: if VIP, the FlowFile goes to the VIP path, otherwise to the standard path." Mention all relevant FlowFile relationships for processors (success, failure, matched/unmatched, etc.) and what those mean in context (e.g., “failures are routed to a separate queue for retry"). This gives the developer insight into error handling and branching logic.
Flowfile Routing and Relationships: Clearly state how the flow branches or merges. For a RouteOnAttribute or similar decision processor, list each route name and the criteria for that route. For example: “RouteOnAttribute ‘EvaluateCustomerType’ has two outcomes – VIPCustomers (condition: customerType == VIP) and OtherCustomers (default route for all others)." Likewise, note if multiple processors merge back together or if a fork in the flow is permanent. Use bullet points to enumerate multiple routes or output streams if that improves clarity. The developer should be able to follow each possible path a FlowFile might take through the process group.
Destinations and Endpoints: For each terminal point (output processor or service), describe where the data ends up. Identify the endpoint by name and type, such as “Data is finally sent to PublishKafka (topic: CustomerUpdates on the corporate Kafka broker)" or “The flow ends with an Output Port Out_To_DataWarehouse which hands off to another NiFi group." This helps the developer understand the integration points and possibly where to look next if this flow connects to others.
Scheduling and Performance Details: Include all scheduling and performance-related configurations. State the Run Schedule for processors (e.g., cron expressions, event-driven, or timer-driven intervals), any Concurrent Tasks set (if a processor allows parallelism, mention how many threads), and Execution Node settings (ALL nodes vs PRIMARY). If back-pressure thresholds or queue sizes are customized on connections, list those as well (e.g., “Connection between X and Y has back pressure set to 5000 objects to throttle ingestion rate"). Mention Load Balancing on connections if configured (e.g., “Using partitioned load balancing on attribute region to distribute data across cluster nodes"). By covering these, a developer can gauge the flow’s throughput and how it’s tuned.
No Broad Strokes – Be Granular: Avoid summarizing multiple processors or actions in one broad statement. Instead of “the data is processed through various steps and then stored," break it down into the individual steps. For example: “Processor A filters out invalid records, then Processor B transforms the data format from XML to JSON, and Processor C aggregates records by customer." Each action gets its own mention. Granularity is key: if the YAML has 10 processors in a chain, the summary should effectively have 10 described steps in the correct order (grouped into paragraphs as needed for readability). This level of detail is critical for someone who might rebuild or debug the flow.
Maintain Technical Accuracy and Terminology: Use correct technical terms and NiFi vocabulary where appropriate so that a developer can directly relate the description to NiFi’s documentation or UI. For example, refer to “FlowFile attributes," “back pressure settings," “Processor yield period," etc., if those concepts come into play. It’s better to be explicitly technical than to risk ambiguity for this audience. If the YAML references specific Controller Services (like a DB Connection Pool or a Schema Registry), include those in the description when relevant (e.g., “queries the EmployeeDB using a DBCPConnectionPool named CorpDBPool for database connectivity").

Example (for context): “Source – ListenHTTP (Port 8080) receives incoming JSON orders. Processor 1 – EvaluateJsonPath extracts customerType and orderTotal from each order. Processor 2 – RouteOnAttribute ‘RouteByCustomerType’: routes FlowFiles to VIP or Standard paths based on the customerType attribute (VIP vs others). VIP path: orders go to PutEmail to notify the VIP service team. Standard path: orders flow into MergeRecord to batch them (max 100 orders per batch). The merge processor is scheduled to run on all nodes every 5 sec (default) and uses the “All Nodes" execution (since it’s okay to parallelize). It has a back pressure threshold of 10000 FlowFiles (default). Processor 3 – PutDatabaseRecord writes the merged orders into the Orders table of the Sales DB. If any insert fails, the FlowFile goes to a failure relationship, which currently terminates (logged for manual review). Output – Upon success, the FlowFile ends at an Output Port Orders_Out for downstream processing.** "
(The above example illustrates the level of detail expected: every component is named and its role explained, including routes and key settings. The developer reading it should have a mental picture of the NiFi canvas with all processors and connections.)
